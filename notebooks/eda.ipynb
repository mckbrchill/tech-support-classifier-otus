{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('../data/complaints-2021-05-14_08_16_.json') \n",
    "  \n",
    "# returns JSON object as a dictionary \n",
    "data = json.load(f)\n",
    "df = pd.json_normalize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['index', 'type', 'id', 'score', 'tags', 'zip_code','complaint_id', 'issue', 'date_received',\n",
    "       'state', 'consumer_disputed', 'product','company_response', 'company', 'submitted_via',\n",
    "       'date_sent_to_company', 'company_public_response','sub_product', 'timely',\n",
    "       'complaint_what_happened', 'sub_issue','consumer_consent_provided']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.loc[:, 'complaint_what_happened'] == ''] = np.nan\n",
    "df = df[~df['complaint_what_happened'].isnull()]\n",
    "df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def clean_text(sent):\n",
    "    sent = sent.lower() # Text to lowercase\n",
    "    pattern = '[^\\w\\s]' # Removing punctuation\n",
    "    sent = re.sub(pattern, '', sent) \n",
    "    pattern = '\\w*\\d\\w*' # Removing words with numbers in between\n",
    "    sent = re.sub(pattern, '', sent) \n",
    "    return sent\n",
    "\n",
    "df['complaint_what_happened'] = df['complaint_what_happened'].astype(str)\n",
    "df_clean = pd.DataFrame(df['complaint_what_happened'].apply(clean_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re, nltk, spacy, string\n",
    "\n",
    "\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# def lemmmatize_text(text):\n",
    "#     sent = []\n",
    "#     doc = nlp(text)\n",
    "#     for token in doc:\n",
    "#         sent.append(token.lemma_)\n",
    "#     return \" \".join(sent)\n",
    "\n",
    "# df_clean['complaint_lemmatized'] = df_clean['complaint_what_happened'].apply(lemmmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# pickle.dump(df_clean, open(\"./artifacts/df_clean_after_encore.pkl\", \"wb\"))\n",
    "\n",
    "with open(\"./artifacts/df_clean_after_encore.pkl\", \"rb\") as file:\n",
    "    df_clean = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m textblob.download_corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "def get_POS_tags(text):\n",
    "    sent = []\n",
    "    blob = TextBlob(text)\n",
    "    sent = [word for (word,tag) in blob.tags if tag=='NN']\n",
    "    return \" \".join(sent)\n",
    "\n",
    "df_clean['complaint_POS_removed'] = df_clean['complaint_lemmatized'].apply(get_POS_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(df_clean, open(\"./artifacts/df_clean_after_pos.pkl\", \"wb\"))\n",
    "\n",
    "# with open(\"./artifacts/df_clean_after_pos.pkl\", \"rb\") as file:\n",
    "#     df_clean = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['Complaint_clean'] = df_clean['complaint_POS_removed'].str.replace('-PRON-', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['Complaint_clean'] = df_clean['Complaint_clean'].str.replace('xxxx','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['Complaint_clean'] = df_clean['Complaint_clean'].str.replace('xxxx','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "\n",
    "tfidf = TfidfVectorizer(min_df=2, max_df=0.95, stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm = tfidf.fit_transform(df_clean['Complaint_clean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load your nmf_model with the n_components i.e 5\n",
    "num_topics = 5\n",
    "\n",
    "#keep the random_state =40\n",
    "nmf_model = NMF(n_components=num_topics, random_state=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_model.fit(dtm)\n",
    "len(tfidf.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = nmf_model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the Top15 words for each of the topics\n",
    "words = np.array(tfidf.get_feature_names_out())\n",
    "topic_words = pd.DataFrame(np.zeros((num_topics, 15)), index=[f'Topic {i + 1}' for i in range(num_topics)],\n",
    "                           columns=[f'Word {i + 1}' for i in range(15)]).astype(str)\n",
    "for i in range(num_topics):\n",
    "    ix = H[i].argsort()[::-1][:15]\n",
    "    topic_words.iloc[i] = words[ix]\n",
    "\n",
    "topic_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the best topic for each complaint in terms of integer value 0,1,2,3 & 4\n",
    "topic_results = nmf_model.transform(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assign the best topic to each of the cmplaints in Topic Column\n",
    "df_clean['Topic'] = topic_results.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the first 5 Complaint for each of the Topics\n",
    "df_clean_5=df_clean.groupby('Topic').head(5)\n",
    "df_clean_5.sort_values('Topic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the dictionary of Topic names and Topics\n",
    "Topic_names = { 0:\"Bank account services\", 1:\"Credit card / Prepaid card\", 2:\"Others\",\n",
    "               3:\"Theft/Dispute reporting\", 4:\"Mortgages/loans\" }\n",
    "#Replace Topics with Topic Names\n",
    "df_clean['Topic'] = df_clean['Topic'].map(Topic_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## PREDICTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the dictionary again of Topic names and Topics\n",
    "Topic_names = { \"Bank account services\":0, \"Credit card / Prepaid card\":1, \"Others\":2,\n",
    "               \"Theft/Dispute reporting\":3, \"Mortgages/loans\":4 }\n",
    "#Replace Topics with Topic Names\n",
    "df_clean['Topic'] = df_clean['Topic'].map(Topic_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = df_clean[['complaint_what_happened', 'Topic']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write your code to get the Vector count\n",
    "vect = CountVectorizer()\n",
    "X_train_cnt = vect.fit_transform(training_data['complaint_what_happened'])\n",
    "\n",
    "# Save word vector\n",
    "pickle.dump(vect.vocabulary_, open(\"count_vector.pk1\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write your code here to transform the word vector to tf-idf\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_cnt)\n",
    "\n",
    "# Save tfidf\n",
    "pickle.dump(tfidf_transformer, open('tfidf.pk1', \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing LogisticRegression from sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# Importing Train, Test Split\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train, Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_tfidf, training_data['Topic'], test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(random_state=42, solver='liblinear').fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the score of the base model\n",
    "logreg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "otus_final_project-6aINOK_7-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
